2016-12-15 13:26:21.002 -[JobGenerator] INFO  kafka.consumer.SimpleConsumer  -Reconnect due to socket error: java.nio.channels.ClosedChannelException
2016-12-15 13:26:42.013 -[JobGenerator] ERROR org.apache.spark.streaming.kafka.DirectKafkaInputDStream  -ArrayBuffer(java.nio.channels.ClosedChannelException)
2016-12-15 13:27:03.211 -[JobGenerator] INFO  kafka.consumer.SimpleConsumer  -Reconnect due to socket error: java.nio.channels.ClosedChannelException
2016-12-15 13:27:24.216 -[JobScheduler] ERROR org.apache.spark.streaming.scheduler.JobScheduler  -Error generating jobs for time 1481779560000 ms
org.apache.spark.SparkException: ArrayBuffer(java.nio.channels.ClosedChannelException)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:123)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:145)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:352)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:352)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:351)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:351)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:346)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:341)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:35)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:352)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:352)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:351)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:351)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:346)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:341)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:47)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:115)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:114)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:114)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:181)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:86)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2016-12-15 13:27:45.219 -[JobGenerator] INFO  kafka.consumer.SimpleConsumer  -Reconnect due to socket error: java.nio.channels.ClosedChannelException
2016-12-15 13:28:06.228 -[JobGenerator] ERROR org.apache.spark.streaming.kafka.DirectKafkaInputDStream  -ArrayBuffer(java.nio.channels.ClosedChannelException)
2016-12-15 15:39:50.260 -[sparkDriverActorSystem-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger  -Slf4jLogger started
2016-12-15 15:39:50.346 -[sparkDriverActorSystem-akka.actor.default-dispatcher-2] INFO  Remoting  -Starting remoting
2016-12-15 15:39:50.558 -[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO  Remoting  -Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.31.154:52782]
2016-12-15 15:39:51.874 -[main] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:39:51.888 -[main] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:39:51.888 -[main] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:39:51.889 -[main] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:39:51.889 -[main] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:39:55.049 -[JobGenerator] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:39:55.050 -[JobGenerator] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:39:55.050 -[JobGenerator] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:39:55.050 -[JobGenerator] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:39:55.050 -[JobGenerator] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:39:55.692 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:39:55.692 -[Executor task launch worker-0] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:39:55.693 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:39:55.693 -[Executor task launch worker-0] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:39:55.693 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:39:55.693 -[Executor task launch worker-0] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:39:55.694 -[Executor task launch worker-1] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:39:55.694 -[Executor task launch worker-0] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:39:55.694 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:39:55.694 -[Executor task launch worker-0] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:39:56.409 -[Executor task launch worker-1] ERROR com.whos.sa.util.log.LogUtil  -java.lang.Exception: org.dom4j.DocumentException: file:\C:\Users\DN\Desktop\Zzz\zg-sa.jar!\com\whos\sa\common\config\config.xml (文件名、目录名或卷标语法不正确。) Nested exception: file:\C:\Users\DN\Desktop\Zzz\zg-sa.jar!\com\whos\sa\common\config\config.xml (文件名、目录名或卷标语法不正确。)
2016-12-15 15:39:56.446 -[Executor task launch worker-1] WARN  net.sf.ehcache.config.ConfigurationFactory  -No configuration found. Configuring ehcache from ehcache-failsafe.xml  found in the classpath: jar:file:/C:/Users/DN/Desktop/Zzz/ehcache-2.9.0.jar!/ehcache-failsafe.xml
2016-12-15 15:39:56.769 -[Executor task launch worker-1] ERROR com.whos.sa.util.log.LogUtil  -java.io.FileNotFoundException: file:\C:\Users\DN\Desktop\Zzz\zg-sa.jar!\com\whos\sa\common\config\dictionary\BosonNLP_sentiment_score.txt (文件名、目录名或卷标语法不正确。)
2016-12-15 15:39:56.780 -[Executor task launch worker-1] INFO  com.chenlb.mmseg4j.Dictionary  -look up in mmseg.dic.path=null
2016-12-15 15:39:56.780 -[Executor task launch worker-1] INFO  com.chenlb.mmseg4j.Dictionary  -look up in classpath=file:/C:/Users/DN/Desktop/Zzz/mmseg4j-core-1.10.0.jar!/data
2016-12-15 15:39:56.781 -[Executor task launch worker-1] WARN  com.chenlb.mmseg4j.Dictionary  -defalut dic path=file:\C:\Users\DN\Desktop\Zzz\mmseg4j-core-1.10.0.jar!\data not exist
2016-12-15 15:39:56.781 -[Executor task launch worker-1] INFO  com.chenlb.mmseg4j.Dictionary  -try to load dir=file:\C:\Users\DN\Desktop\Zzz\mmseg4j-core-1.10.0.jar!\data
2016-12-15 15:39:56.795 -[Executor task launch worker-1] INFO  com.chenlb.mmseg4j.Dictionary  -chars loaded time=13ms, line=12638, on file=file:\C:\Users\DN\Desktop\Zzz\mmseg4j-core-1.10.0.jar!\data\chars.dic
2016-12-15 15:39:56.902 -[Executor task launch worker-1] INFO  com.chenlb.mmseg4j.Dictionary  -words loaded time=106ms, line=149853, on file=file:\C:\Users\DN\Desktop\Zzz\mmseg4j-core-1.10.0.jar!\data\words.dic
2016-12-15 15:39:56.903 -[Executor task launch worker-1] INFO  com.chenlb.mmseg4j.Dictionary  -load all dic use time=121ms
2016-12-15 15:39:56.904 -[Executor task launch worker-1] INFO  com.chenlb.mmseg4j.Dictionary  -unit loaded time=0ms, line=22, on file=file:\C:\Users\DN\Desktop\Zzz\mmseg4j-core-1.10.0.jar!\data\units.dic
2016-12-15 15:39:56.974 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:39:56.975 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:39:56.975 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:39:56.975 -[Executor task launch worker-1] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:39:56.975 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:39:56.997 -[Executor task launch worker-1] INFO  com.chenlb.mmseg4j.Dictionary  -try to load dir=file:\C:\Users\DN\Desktop\Zzz\mmseg4j-core-1.10.0.jar!\data
2016-12-15 15:39:59.074 -[Executor task launch worker-0] INFO  com.chenlb.mmseg4j.Dictionary  -try to load dir=file:\C:\Users\DN\Desktop\Zzz\mmseg4j-core-1.10.0.jar!\data
2016-12-15 15:44:32.971 -[main] INFO  org.apache.spark.SparkContext  -Running Spark version 1.6.1
2016-12-15 15:44:33.224 -[main] WARN  org.apache.hadoop.util.NativeCodeLoader  -Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-12-15 15:44:33.331 -[main] INFO  org.apache.spark.SecurityManager  -Changing view acls to: cluster
2016-12-15 15:44:33.331 -[main] INFO  org.apache.spark.SecurityManager  -Changing modify acls to: cluster
2016-12-15 15:44:33.332 -[main] INFO  org.apache.spark.SecurityManager  -SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cluster); users with modify permissions: Set(cluster)
2016-12-15 15:44:33.855 -[main] INFO  org.apache.spark.util.Utils  -Successfully started service 'sparkDriver' on port 53283.
2016-12-15 15:44:34.099 -[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger  -Slf4jLogger started
2016-12-15 15:44:34.131 -[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO  Remoting  -Starting remoting
2016-12-15 15:44:34.276 -[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO  Remoting  -Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.31.154:53296]
2016-12-15 15:44:34.280 -[main] INFO  org.apache.spark.util.Utils  -Successfully started service 'sparkDriverActorSystem' on port 53296.
2016-12-15 15:44:34.298 -[main] INFO  org.apache.spark.SparkEnv  -Registering MapOutputTracker
2016-12-15 15:44:34.312 -[main] INFO  org.apache.spark.SparkEnv  -Registering BlockManagerMaster
2016-12-15 15:44:34.324 -[main] INFO  org.apache.spark.storage.DiskBlockManager  -Created local directory at C:\Users\DN\AppData\Local\Temp\blockmgr-4044a7cb-e5a1-48e2-beed-5465b090fc16
2016-12-15 15:44:34.332 -[main] INFO  org.apache.spark.storage.MemoryStore  -MemoryStore started with capacity 517.4 MB
2016-12-15 15:44:34.372 -[main] INFO  org.apache.spark.SparkEnv  -Registering OutputCommitCoordinator
2016-12-15 15:44:34.489 -[main] INFO  org.spark-project.jetty.server.Server  -jetty-8.y.z-SNAPSHOT
2016-12-15 15:44:34.521 -[main] WARN  org.spark-project.jetty.util.component.AbstractLifeCycle  -FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use: bind
java.net.BindException: Address already in use: bind
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1988)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1979)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:874)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:81)
	at sparkMySQL_weibo_emotion$.main(sparkMySQL_weibo_emotion.scala:38)
	at sparkMySQL_weibo_emotion.main(sparkMySQL_weibo_emotion.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
2016-12-15 15:44:34.523 -[main] WARN  org.spark-project.jetty.util.component.AbstractLifeCycle  -FAILED org.spark-project.jetty.server.Server@9c413e: java.net.BindException: Address already in use: bind
java.net.BindException: Address already in use: bind
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1988)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1979)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:874)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:81)
	at sparkMySQL_weibo_emotion$.main(sparkMySQL_weibo_emotion.scala:38)
	at sparkMySQL_weibo_emotion.main(sparkMySQL_weibo_emotion.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
2016-12-15 15:44:34.538 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-12-15 15:44:34.538 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/api,null}
2016-12-15 15:44:34.538 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/,null}
2016-12-15 15:44:34.538 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/static,null}
2016-12-15 15:44:34.538 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-12-15 15:44:34.539 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-12-15 15:44:34.539 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-12-15 15:44:34.539 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-12-15 15:44:34.539 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-12-15 15:44:34.540 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-12-15 15:44:34.540 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-12-15 15:44:34.540 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-12-15 15:44:34.540 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-12-15 15:44:34.540 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-12-15 15:44:34.540 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-12-15 15:44:34.541 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-12-15 15:44:34.541 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-12-15 15:44:34.542 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-12-15 15:44:34.542 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-12-15 15:44:34.542 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-12-15 15:44:34.542 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-12-15 15:44:34.543 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-12-15 15:44:34.543 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-12-15 15:44:34.543 -[main] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-12-15 15:44:34.607 -[main] WARN  org.apache.spark.util.Utils  -Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2016-12-15 15:44:34.610 -[main] INFO  org.spark-project.jetty.server.Server  -jetty-8.y.z-SNAPSHOT
2016-12-15 15:44:34.639 -[main] INFO  org.spark-project.jetty.server.AbstractConnector  -Started SelectChannelConnector@0.0.0.0:4041
2016-12-15 15:44:34.639 -[main] INFO  org.apache.spark.util.Utils  -Successfully started service 'SparkUI' on port 4041.
2016-12-15 15:44:34.641 -[main] INFO  org.apache.spark.ui.SparkUI  -Started SparkUI at http://192.168.31.154:4041
2016-12-15 15:44:34.710 -[main] INFO  org.apache.spark.executor.Executor  -Starting executor ID driver on host localhost
2016-12-15 15:44:34.729 -[main] INFO  org.apache.spark.util.Utils  -Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53307.
2016-12-15 15:44:34.729 -[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  -Server created on 53307
2016-12-15 15:44:34.731 -[main] INFO  org.apache.spark.storage.BlockManagerMaster  -Trying to register BlockManager
2016-12-15 15:44:34.733 -[dispatcher-event-loop-2] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  -Registering block manager localhost:53307 with 517.4 MB RAM, BlockManagerId(driver, localhost, 53307)
2016-12-15 15:44:34.734 -[main] INFO  org.apache.spark.storage.BlockManagerMaster  -Registered BlockManager
2016-12-15 15:44:34.995 -[main] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:44:35.002 -[main] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:44:35.003 -[main] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:44:35.003 -[main] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:44:35.003 -[main] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:44:35.426 -[streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream  -metadataCleanupDelay = -1
2016-12-15 15:44:35.427 -[streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream  -metadataCleanupDelay = -1
2016-12-15 15:44:35.428 -[streaming-start] INFO  org.apache.spark.streaming.kafka.DirectKafkaInputDStream  -metadataCleanupDelay = -1
2016-12-15 15:44:35.428 -[streaming-start] INFO  org.apache.spark.streaming.kafka.DirectKafkaInputDStream  -Slide time = 5000 ms
2016-12-15 15:44:35.429 -[streaming-start] INFO  org.apache.spark.streaming.kafka.DirectKafkaInputDStream  -Storage level = StorageLevel(false, false, false, false, 1)
2016-12-15 15:44:35.429 -[streaming-start] INFO  org.apache.spark.streaming.kafka.DirectKafkaInputDStream  -Checkpoint interval = null
2016-12-15 15:44:35.430 -[streaming-start] INFO  org.apache.spark.streaming.kafka.DirectKafkaInputDStream  -Remember duration = 5000 ms
2016-12-15 15:44:35.430 -[streaming-start] INFO  org.apache.spark.streaming.kafka.DirectKafkaInputDStream  -Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@1d50cd7
2016-12-15 15:44:35.430 -[streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream  -Slide time = 5000 ms
2016-12-15 15:44:35.430 -[streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream  -Storage level = StorageLevel(false, false, false, false, 1)
2016-12-15 15:44:35.430 -[streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream  -Checkpoint interval = null
2016-12-15 15:44:35.430 -[streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream  -Remember duration = 5000 ms
2016-12-15 15:44:35.430 -[streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream  -Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@2459f1
2016-12-15 15:44:35.431 -[streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream  -Slide time = 5000 ms
2016-12-15 15:44:35.431 -[streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream  -Storage level = StorageLevel(false, false, false, false, 1)
2016-12-15 15:44:35.431 -[streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream  -Checkpoint interval = null
2016-12-15 15:44:35.431 -[streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream  -Remember duration = 5000 ms
2016-12-15 15:44:35.431 -[streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream  -Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@74b6d7
2016-12-15 15:44:35.493 -[streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer  -Started timer for JobGenerator at time 1481787880000
2016-12-15 15:44:35.494 -[streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator  -Started JobGenerator at 1481787880000 ms
2016-12-15 15:44:35.496 -[streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler  -Started JobScheduler
2016-12-15 15:44:35.500 -[main] INFO  org.apache.spark.streaming.StreamingContext  -StreamingContext started
2016-12-15 15:44:40.074 -[JobGenerator] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:44:40.074 -[JobGenerator] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:44:40.074 -[JobGenerator] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:44:40.075 -[JobGenerator] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:44:40.075 -[JobGenerator] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:44:40.106 -[JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler  -Added jobs for time 1481787880000 ms
2016-12-15 15:44:40.108 -[JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler  -Starting job streaming job 1481787880000 ms.0 from job set of time 1481787880000 ms
2016-12-15 15:44:40.127 -[streaming-job-executor-0] INFO  org.apache.spark.SparkContext  -Starting job: foreachPartition at sparkMySQL_weibo_emotion.scala:55
2016-12-15 15:44:40.138 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler  -Got job 0 (foreachPartition at sparkMySQL_weibo_emotion.scala:55) with 3 output partitions
2016-12-15 15:44:40.138 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler  -Final stage: ResultStage 0 (foreachPartition at sparkMySQL_weibo_emotion.scala:55)
2016-12-15 15:44:40.139 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler  -Parents of final stage: List()
2016-12-15 15:44:40.140 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler  -Missing parents: List()
2016-12-15 15:44:40.148 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler  -Submitting ResultStage 0 (MapPartitionsRDD[1] at map at sparkMySQL_weibo_emotion.scala:48), which has no missing parents
2016-12-15 15:44:40.259 -[dag-scheduler-event-loop] WARN  org.apache.spark.util.SizeEstimator  -Failed to check whether UseCompressedOops is set; assuming yes
2016-12-15 15:44:40.265 -[dag-scheduler-event-loop] INFO  org.apache.spark.storage.MemoryStore  -Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 3.1 KB)
2016-12-15 15:44:40.276 -[dag-scheduler-event-loop] INFO  org.apache.spark.storage.MemoryStore  -Block broadcast_0_piece0 stored as bytes in memory (estimated size 1822.0 B, free 4.9 KB)
2016-12-15 15:44:40.278 -[dispatcher-event-loop-0] INFO  org.apache.spark.storage.BlockManagerInfo  -Added broadcast_0_piece0 in memory on localhost:53307 (size: 1822.0 B, free: 517.4 MB)
2016-12-15 15:44:40.280 -[dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext  -Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2016-12-15 15:44:40.283 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler  -Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at sparkMySQL_weibo_emotion.scala:48)
2016-12-15 15:44:40.285 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl  -Adding task set 0.0 with 3 tasks
2016-12-15 15:44:40.318 -[dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager  -Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,ANY, 2020 bytes)
2016-12-15 15:44:40.321 -[dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager  -Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,ANY, 2020 bytes)
2016-12-15 15:44:40.325 -[Executor task launch worker-0] INFO  org.apache.spark.executor.Executor  -Running task 0.0 in stage 0.0 (TID 0)
2016-12-15 15:44:40.325 -[Executor task launch worker-1] INFO  org.apache.spark.executor.Executor  -Running task 1.0 in stage 0.0 (TID 1)
2016-12-15 15:44:40.348 -[Executor task launch worker-0] INFO  org.apache.spark.streaming.kafka.KafkaRDD  -Computing topic weibo_content2, partition 0 offsets 0 -> 259
2016-12-15 15:44:40.348 -[Executor task launch worker-1] INFO  org.apache.spark.streaming.kafka.KafkaRDD  -Computing topic weibo_content2, partition 1 offsets 0 -> 271
2016-12-15 15:44:40.348 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:44:40.348 -[Executor task launch worker-0] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:44:40.348 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:44:40.348 -[Executor task launch worker-0] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:44:40.348 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:44:40.348 -[Executor task launch worker-0] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:44:40.349 -[Executor task launch worker-1] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:44:40.349 -[Executor task launch worker-0] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:44:40.349 -[Executor task launch worker-1] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:44:40.349 -[Executor task launch worker-0] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:44:40.713 -[Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  -Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NoClassDefFoundError: Could not initialize class com.whos.sa.common.Config
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-12-15 15:44:40.713 -[Executor task launch worker-1] ERROR org.apache.spark.executor.Executor  -Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NoClassDefFoundError: org/dom4j/DocumentException
	at com.whos.sa.common.Config.loadConfig(Config.java:32)
	at com.whos.sa.common.Config.<init>(Config.java:22)
	at com.whos.sa.common.Config.<clinit>(Config.java:17)
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.dom4j.DocumentException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more
2016-12-15 15:44:40.734 -[Executor task launch worker-1] ERROR org.apache.spark.util.SparkUncaughtExceptionHandler  -Uncaught exception in thread Thread[Executor task launch worker-1,5,main]
java.lang.NoClassDefFoundError: org/dom4j/DocumentException
	at com.whos.sa.common.Config.loadConfig(Config.java:32)
	at com.whos.sa.common.Config.<init>(Config.java:22)
	at com.whos.sa.common.Config.<clinit>(Config.java:17)
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.dom4j.DocumentException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more
2016-12-15 15:44:40.734 -[Executor task launch worker-0] ERROR org.apache.spark.util.SparkUncaughtExceptionHandler  -Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.NoClassDefFoundError: Could not initialize class com.whos.sa.common.Config
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-12-15 15:44:40.735 -[dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager  -Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2,ANY, 2020 bytes)
2016-12-15 15:44:40.737 -[Thread-0] INFO  org.apache.spark.streaming.StreamingContext  -Invoking stop(stopGracefully=false) from shutdown hook
2016-12-15 15:44:40.738 -[Thread-0] INFO  org.apache.spark.streaming.scheduler.JobGenerator  -Stopping JobGenerator immediately
2016-12-15 15:44:40.738 -[Executor task launch worker-2] INFO  org.apache.spark.executor.Executor  -Running task 2.0 in stage 0.0 (TID 2)
2016-12-15 15:44:40.739 -[Thread-0] INFO  org.apache.spark.streaming.util.RecurringTimer  -Stopped timer for JobGenerator after time 1481787880000
2016-12-15 15:44:40.740 -[Executor task launch worker-2] INFO  org.apache.spark.streaming.kafka.KafkaRDD  -Computing topic weibo_content2, partition 2 offsets 0 -> 264
2016-12-15 15:44:40.741 -[Executor task launch worker-2] INFO  kafka.utils.VerifiableProperties  -Verifying properties
2016-12-15 15:44:40.741 -[Executor task launch worker-2] INFO  kafka.utils.VerifiableProperties  -Property auto.offset.reset is overridden to smallest
2016-12-15 15:44:40.741 -[Executor task launch worker-2] INFO  kafka.utils.VerifiableProperties  -Property group.id is overridden to 
2016-12-15 15:44:40.741 -[Executor task launch worker-2] WARN  kafka.utils.VerifiableProperties  -Property serializer.class is not valid
2016-12-15 15:44:40.741 -[Executor task launch worker-2] INFO  kafka.utils.VerifiableProperties  -Property zookeeper.connect is overridden to 
2016-12-15 15:44:40.744 -[Thread-0] INFO  org.apache.spark.streaming.scheduler.JobGenerator  -Stopped JobGenerator
2016-12-15 15:44:40.744 -[task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager  -Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NoClassDefFoundError: org/dom4j/DocumentException
	at com.whos.sa.common.Config.loadConfig(Config.java:32)
	at com.whos.sa.common.Config.<init>(Config.java:22)
	at com.whos.sa.common.Config.<clinit>(Config.java:17)
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.dom4j.DocumentException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more

2016-12-15 15:44:40.746 -[task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager  -Task 1 in stage 0.0 failed 1 times; aborting job
2016-12-15 15:44:40.747 -[task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager  -Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NoClassDefFoundError: Could not initialize class com.whos.sa.common.Config
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2016-12-15 15:44:40.752 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl  -Cancelling stage 0
2016-12-15 15:44:40.755 -[dispatcher-event-loop-2] INFO  org.apache.spark.executor.Executor  -Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
2016-12-15 15:44:40.756 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl  -Stage 0 was cancelled
2016-12-15 15:44:40.757 -[dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler  -ResultStage 0 (foreachPartition at sparkMySQL_weibo_emotion.scala:55) failed in 0.459 s
2016-12-15 15:44:40.758 -[streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler  -Job 0 failed: foreachPartition at sparkMySQL_weibo_emotion.scala:55, took 0.631011 s
2016-12-15 15:44:40.762 -[JobScheduler] ERROR org.apache.spark.streaming.scheduler.StreamingListenerBus  -StreamingListenerBus has already stopped! Dropping event StreamingListenerOutputOperationCompleted(OutputOperationInfo(1481787880000 ms,0,foreachRDD at sparkMySQL_weibo_emotion.scala:54,org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:659)
sparkMySQL_weibo_emotion$.main(sparkMySQL_weibo_emotion.scala:54)
sparkMySQL_weibo_emotion.main(sparkMySQL_weibo_emotion.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
com.intellij.rt.execution.application.AppMain.main(AppMain.java:144),Some(1481787880107),Some(1481787880760),Some(org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NoClassDefFoundError: org/dom4j/DocumentException
	at com.whos.sa.common.Config.loadConfig(Config.java:32)
	at com.whos.sa.common.Config.<init>(Config.java:22)
	at com.whos.sa.common.Config.<clinit>(Config.java:17)
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.dom4j.DocumentException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:918)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:918)
	at sparkMySQL_weibo_emotion$$anonfun$main$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at sparkMySQL_weibo_emotion$$anonfun$main$1.apply(sparkMySQL_weibo_emotion.scala:54)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:49)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:224)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:223)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: org/dom4j/DocumentException
	at com.whos.sa.common.Config.loadConfig(Config.java:32)
	at com.whos.sa.common.Config.<init>(Config.java:22)
	at com.whos.sa.common.Config.<clinit>(Config.java:17)
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
Caused by: java.lang.ClassNotFoundException: org.dom4j.DocumentException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more
)))
2016-12-15 15:44:40.767 -[JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler  -Finished job streaming job 1481787880000 ms.0 from job set of time 1481787880000 ms
2016-12-15 15:44:40.768 -[JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler  -Total delay: 0.760 s for time 1481787880000 ms (execution: 0.653 s)
2016-12-15 15:44:40.769 -[JobScheduler] ERROR org.apache.spark.streaming.scheduler.StreamingListenerBus  -StreamingListenerBus has already stopped! Dropping event StreamingListenerBatchCompleted(BatchInfo(1481787880000 ms,Map(0 -> StreamInputInfo(0,794,Map(offsets -> List(OffsetRange(topic: 'weibo_content2', partition: 0, range: [0 -> 259]), OffsetRange(topic: 'weibo_content2', partition: 1, range: [0 -> 271]), OffsetRange(topic: 'weibo_content2', partition: 2, range: [0 -> 264])), Description -> topic: weibo_content2	partition: 0	offsets: 0 to 259
topic: weibo_content2	partition: 1	offsets: 0 to 271
topic: weibo_content2	partition: 2	offsets: 0 to 264))),1481787880101,Some(1481787880107),Some(1481787880760),Map(0 -> OutputOperationInfo(1481787880000 ms,0,foreachRDD at sparkMySQL_weibo_emotion.scala:54,org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:659)
sparkMySQL_weibo_emotion$.main(sparkMySQL_weibo_emotion.scala:54)
sparkMySQL_weibo_emotion.main(sparkMySQL_weibo_emotion.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
com.intellij.rt.execution.application.AppMain.main(AppMain.java:144),Some(1481787880107),Some(1481787880760),Some(org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NoClassDefFoundError: org/dom4j/DocumentException
	at com.whos.sa.common.Config.loadConfig(Config.java:32)
	at com.whos.sa.common.Config.<init>(Config.java:22)
	at com.whos.sa.common.Config.<clinit>(Config.java:17)
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.dom4j.DocumentException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:918)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:918)
	at sparkMySQL_weibo_emotion$$anonfun$main$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at sparkMySQL_weibo_emotion$$anonfun$main$1.apply(sparkMySQL_weibo_emotion.scala:54)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:49)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:224)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:223)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: org/dom4j/DocumentException
	at com.whos.sa.common.Config.loadConfig(Config.java:32)
	at com.whos.sa.common.Config.<init>(Config.java:22)
	at com.whos.sa.common.Config.<clinit>(Config.java:17)
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
Caused by: java.lang.ClassNotFoundException: org.dom4j.DocumentException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more
)))))
2016-12-15 15:44:40.771 -[Thread-0] INFO  org.apache.spark.streaming.scheduler.JobScheduler  -Stopped JobScheduler
2016-12-15 15:44:40.773 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/streaming,null}
2016-12-15 15:44:40.774 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/streaming/batch,null}
2016-12-15 15:44:40.775 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/static/streaming,null}
2016-12-15 15:44:40.776 -[Thread-0] INFO  org.apache.spark.streaming.StreamingContext  -StreamingContext stopped successfully
2016-12-15 15:44:40.776 -[Thread-0] INFO  org.apache.spark.SparkContext  -Invoking stop() from shutdown hook
2016-12-15 15:44:40.779 -[Executor task launch worker-2] ERROR org.apache.spark.executor.Executor  -Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.NoClassDefFoundError: Could not initialize class com.whos.sa.common.Config
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-12-15 15:44:40.781 -[Executor task launch worker-2] ERROR org.apache.spark.util.SparkUncaughtExceptionHandler  -[Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-2,5,main]
java.lang.NoClassDefFoundError: Could not initialize class com.whos.sa.common.Config
	at com.whos.sa.analysis.Analysis.<init>(Analysis.java:19)
	at sparkMySQL_weibo_emotion$.Analysis_emotion(sparkMySQL_weibo_emotion.scala:21)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:51)
	at sparkMySQL_weibo_emotion$$anonfun$1.apply(sparkMySQL_weibo_emotion.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:62)
	at sparkMySQL_weibo_emotion$$anonfun$main$1$$anonfun$apply$1.apply(sparkMySQL_weibo_emotion.scala:55)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-12-15 15:44:40.783 -[task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager  -Lost task 2.0 in stage 0.0 (TID 2) on executor localhost: java.lang.NoClassDefFoundError (Could not initialize class com.whos.sa.common.Config) [duplicate 1]
2016-12-15 15:44:40.784 -[task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl  -Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-12-15 15:44:40.788 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/streaming/batch/json,null}
2016-12-15 15:44:40.788 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/streaming/json,null}
2016-12-15 15:44:40.788 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-12-15 15:44:40.788 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/api,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/static,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-12-15 15:44:40.789 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-12-15 15:44:40.790 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-12-15 15:44:40.791 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-12-15 15:44:40.791 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-12-15 15:44:40.791 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-12-15 15:44:40.791 -[Thread-0] INFO  org.spark-project.jetty.server.handler.ContextHandler  -stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-12-15 15:44:40.845 -[Thread-0] INFO  org.apache.spark.ui.SparkUI  -Stopped Spark web UI at http://192.168.31.154:4041
2016-12-15 15:44:40.868 -[dispatcher-event-loop-2] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  -MapOutputTrackerMasterEndpoint stopped!
2016-12-15 15:44:40.879 -[Thread-0] INFO  org.apache.spark.storage.MemoryStore  -MemoryStore cleared
2016-12-15 15:44:40.880 -[Thread-0] INFO  org.apache.spark.storage.BlockManager  -BlockManager stopped
2016-12-15 15:44:40.890 -[Thread-0] INFO  org.apache.spark.storage.BlockManagerMaster  -BlockManagerMaster stopped
2016-12-15 15:44:40.895 -[dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  -OutputCommitCoordinator stopped!
2016-12-15 15:44:40.898 -[Thread-0] INFO  org.apache.spark.SparkContext  -Successfully stopped SparkContext
2016-12-15 15:44:40.899 -[Thread-0] INFO  org.apache.spark.util.ShutdownHookManager  -Shutdown hook called
2016-12-15 15:44:40.899 -[Thread-0] INFO  org.apache.spark.util.ShutdownHookManager  -Deleting directory C:\Users\DN\AppData\Local\Temp\spark-30426718-99e6-4645-b5e5-8eba00ce53ca
